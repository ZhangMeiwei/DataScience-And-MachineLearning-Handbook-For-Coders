# Introduction

EM 算法是机器学习所谓的十大算法之一，它很简单，又很复杂。简单在于它的思想，简单在于其仅包含了两个步骤就能完成强大的功能，复杂在于它的数学推理涉及到比较繁杂的概率公式等。EM 算法属于聚类算法之一，而最简单的聚类算法当属 K-Means 算法，但是 K-Means 算法无法计算某个样本属于该簇的后验概率。

## Reference

* [EM 算法原理和应用](http://www.algorithmdog.com/em%E7%AE%97%E6%B3%95%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%BC%95%E5%85%A5?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io)
  ## Mathematics
  ### 极大似然
  假设我们需要调查我们学校的男生和女生的身高分布。你怎么做啊？你说那么多人不可能一个一个去问吧，肯定是抽样了。假设你在校园里随便地活捉了 100 个男生和 100 个女生。他们共 200 个人（也就是 200 个身高的样本数据，为了方便表示，下面，我说 “ 人 ” 的意思就是对应的身高）都在教室里面了。那下一步怎么办啊？你开始喊：“ 男的左边，女的右边，其他的站中间！” 。然后你就先统计抽样得到的 100 个男生的身高。假设他们的身高是服从高斯分布的。但是这个分布的均值 u 和方差 ∂2 我们不知道，这两个参数就是我们要估计的。记作 θ=[u, ∂]T。用数学的语言来说就是：在学校那么多男生（身高）中，我们独立地按照概率密度 p(x|θ) 抽取 100 了个（身高），组成样本集 X，我们想通过样本集 X 来估计出未知参数 θ。这里概率密度 p(x|θ) 我们知道了是高斯分布 N(u,∂) 的形式，其中的未知参数是 θ=[u, ∂]T。抽到的样本集是 X={x1,x2,…,xN}，其中 xi 表示抽到的第 i 个人的身高，这里 N 就是 100，表示抽到的样本个数。由于每个样本都是独立地从 p(x|θ) 中抽取的，换句话说这 100 个男生中的任何一个，都是我随便捉的，从我的角度来看这些男生之间是没有关系的。那么，我从学校那么多男生中为什么就恰好抽到了这 100 个人呢？抽到这 100 个人的概率是多少呢？因为这些男生（的身高）是服从同一个高斯分布 p(x|θ) 的。那么我抽到男生 A（的身高）的概率是 p(xA|θ)，抽到男生 B 的概率是 p(xB|θ)，那因为他们是独立的，所以很明显，我同时抽到男生 A 和男生 B 的概率是 p(xA|θ)\* p(xB|θ)，同理，我同时抽到这 100 个男生的概率就是他们各自概率的乘积了。用数学家的口吻说就是从分布是 p(x|θ) 的总体样本中抽取到这 100 个样本的概率，也就是样本集 X 中各个样本的联合概率，用下式表示： ![](http://img.my.csdn.net/uploads/201301/24/1359003923_8916.jpg) 这个概率反映了，在概率密度函数的参数是 θ 时，得到 X 这组样本的概率。因为这里 X 是已知的，也就是说我抽取到的这 100 个人的身高可以测出来，也就是已知的了。而 θ 是未知了，则上面这个公式只有 θ 是未知数，所以它是 θ 的函数。这个函数放映的是在不同的参数 θ 取值下，取得当前这个样本集的可能性，因此称为参数 θ 相对于样本集 X 的似然函数（likehood function ）。记为 L(θ)。在学校那么男生中，我一抽就抽到这 100 个男生（表示身高），而不是其他人，那是不是表示在整个学校中，这 100 个人（的身高）出现的概率最大啊。那么这个概率怎么表示？哦，就是上面那个似然函数 L(θ)。所以，我们就只需要找到一个参数 θ，其对应的似然函数 L(θ) 最大，也就是说抽到这 100 个男生（的身高）概率最大。这个叫做 θ 的最大似然估计量，记为： ![](http://img.my.csdn.net/uploads/201301/24/1359003973_1560.jpg) 有时，可以看到 L(θ) 是连乘的，所以为了便于分析，还可以定义对数似然函数，将其变成连加的： ![](http://img.my.csdn.net/uploads/201301/24/1359003994_1029.jpg) 好了，现在我们知道了，要求 θ，只需要使 θ 的似然函数 L(θ) 极大化，然后极大值对应的 θ 就是我们的估计。这里就回到了求最值的问题了。怎么求一个函数的最值？当然是求导，然后让导数为 0，那么解这个方程得到的 θ 就是了（当然，前提是函数 L(θ) 连续可微）。那如果 θ 是包含多个参数的向量那怎么处理啊？当然是求 L(θ) 对所有参数的偏导数，也就是梯度了，那么 n 个未知的参数，就有 n 个方程，方程组的解就是似然函数的极值点了，当然就得到这 n 个参数了。求最大似然函数估计值的一般步骤：（ 1）写出似然函数；（ 2）对似然函数取对数，并整理；（ 3）求导数，令导数为 0，得到似然方程；（ 4）解似然方程，得到的参数即为所求；

### Jensen 不等式

在 $f$ 是凸函数的情况下，基本的 Jensen 不等式为 :$$f(\theta x + (1-\theta)y) \le \theta f(x) + (1 - \theta)f(y)$$ ![](http://7xlgth.com1.z0.glb.clouddn.com/B771D505-1979-4D5B-827C-FC56BF0F9CB2.png) 也就是 $$f(Ex) \le Ef(x)$$ ![](http://img.my.csdn.net/uploads/201301/24/1359004230_7889.jpg) 图中，实线 f 是凸函数，X 是随机变量，有 0.5 的概率是 a，有 0.5 的概率是 b。（就像掷硬币一样）。 X 的期望值就是 a 和 b 的中值了，图中可以看到 E[f(X)]>=f(E[X]) 成立。

## Reference

* [从最大似然到 EM 算法浅解](http://blog.csdn.net/zouxy09/article/details/8537620)
  # EM 算法
  现在，通过抽取得到的那 100 个男生的身高和已知的其身高服从高斯分布，我们通过最大化其似然函数，就可以得到了对应高斯分布的参数 θ=[u, ∂]T 了。那么，对于我们学校的女生的身高分布也可以用同样的方法得到了。 再回到例子本身，如果没有 “ 男的左边，女的右边，其他的站中间！” 这个步骤，或者说我抽到这 200 个人中，某些男生和某些女生一见钟情，已经好上了，纠缠起来了。咱们也不想那么残忍，硬把他们拉扯开。那现在这 200 个人已经混到一起了，这时候，你从这 200 个人（的身高）里面随便给我指一个人（的身高），我都无法确定这个人（的身高）是男生（的身高）还是女生（的身高）。也就是说你不知道抽取的那 200 个人里面的每一个人到底是从男生的那个身高分布里面抽取的，还是女生的那个身高分布抽取的。用数学的语言就是，抽取得到的每个样本都不知道是从哪个分布抽取的。 这个时候，对于每一个样本或者你抽取到的人，就有两个东西需要猜测或者估计的了，一是这个人是男的还是女的？二是男生和女生对应的身高的高斯分布的参数是多少？ EM 算法就是这样，假设我们想估计知道 A 和 B 两个参数，在开始状态下二者都是未知的，但如果知道了 A 的信息就可以得到 B 的信息，反过来知道了 B 也就得到了 A。可以考虑首先赋予 A 某种初值，以此得到 B 的估计值，然后从 B 的当前值出发，重新估计 A 的取值，这个过程一直持续到收敛为止。 EM 的意思是 “Expectation Maximization”，在我们上面这个问题里面，我们是先随便猜一下男生（身高）的正态分布的参数：如均值和方差是多少。例如男生的均值是 1 米 7，方差是 0.1 米（当然了，刚开始肯定没那么准），然后计算出每个人更可能属于第一个还是第二个正态分布中的（例如，这个人的身高是 1 米 8，那很明显，他最大可能属于男生的那个分布），这个是属于 Expectation 一步。有了每个人的归属，或者说我们已经大概地按上面的方法将这 200 个人分为男生和女生两部分，我们就可以根据之前说的最大似然那样，通过这些被大概分为男生的 n 个人来重新估计第一个分布的参数，女生的那个分布同样方法重新估计。这个是 Maximization。然后，当我们更新了这两个分布的时候，每一个属于这两个分布的概率又变了，那么我们就再需要调整 E 步 …… 如此往复，直到参数基本不再发生变化为止。 这里把每个人（样本）的完整描述看做是三元组 yi={xi,zi1,zi2}，其中，xi 是第 i 个样本的观测值，也就是对应的这个人的身高，是可以观测到的值。zi1 和 zi2 表示男生和女生这两个高斯分布中哪个被用来产生值 xi，就是说这两个值标记这个人到底是男生还是女生（的身高分布产生的）。这两个值我们是不知道的，是隐含变量。确切的说，zij 在 xi 由第 j 个高斯分布产生时值为 1，否则为 0。例如一个样本的观测值为 1.8，然后他来自男生的那个高斯分布，那么我们可以将这个样本表示为 {1.8, 1, 0}。如果 zi1 和 zi2 的值已知，也就是说每个人我已经标记为男生或者女生了，那么我们就可以利用上面说的最大似然算法来估计他们各自高斯分布的参数。但是它们未知，因此我们只能用 EM 算法。
  ## EM 算法推导
  假设我们有一个样本集 {x(1),…,x(m)}，包含 m 个独立的样本。但每个样本 i 对应的类别 z(i) 是未知的（相当于聚类），也即隐含变量。故我们需要估计概率模型 p(x,z) 的参数 θ，但是由于里面包含隐含变量 z，所以很难用最大似然求解，但如果 z 知道了，那我们就很容易求解了。 对于参数估计，我们本质上还是想获得一个使似然函数最大化的那个参数 θ，现在与最大似然不同的只是似然函数式中多了一个未知的变量 z，见下式（1 ）。也就是说我们的目标是找到适合的 θ 和 z 让 L(θ) 最大。那我们也许会想，你就是多了一个未知的变量而已啊，我也可以分别对未知的 θ 和 z 分别求偏导，再令其等于 0，求解出来不也一样吗？ ![](http://img.my.csdn.net/uploads/201301/24/1359004165_6698.jpg) 本质上我们是需要最大化（1 ）式（对（1 ）式，我们回忆下联合概率密度下某个变量的边缘概率密度函数的求解，注意这里 z 也是随机变量。对每一个样本 i 的所有可能类别 z 求等式右边的联合概率密度函数和，也就得到等式左边为随机变量 x 的边缘概率密度），也就是似然函数，但是可以看到里面有 “ 和的对数 ”，求导后形式会非常复杂（自己可以想象下 log(f1(x)+ f2(x)+ f3(x)+…) 复合函数的求导），所以很难求解得到未知参数 z 和 θ。那 OK，我们可否对（1 ）式做一些改变呢？我们看（2 ）式，（ 2）式只是分子分母同乘以一个相等的函数，还是有 “ 和的对数 ” 啊，还是求解不了，那为什么要这么做呢？咱们先不管，看（3 ）式，发现（3 ）式变成了 “ 对数的和 ”，那这样求导就容易了。我们注意点，还发现等号变成了不等号，为什么能这么变呢？这就是 Jensen 不等式的大显神威的地方。回到公式（2 ），因为 f(x)=log x 为凹函数（其二次导数为 $\frac{-1}{x^2} < 0$）。 2）式中 ![](http://img.my.csdn.net/uploads/201301/24/1359004420_6093.jpg) 的期望，（考虑到 E(X)=∑x*p(x)，f(X) 是 X 的函数，则 E(f(X))=∑f(x)*p(x)），又 ![](http://img.my.csdn.net/uploads/201301/24/1359004435_1667.jpg)，所以就可以得到公式（3 ）的不等式了（若不明白，请拿起笔，呵呵）： OK，到这里，现在式（3 ）就容易地求导了，但是式（2 ）和式（3 ）是不等号啊，式（2 ）的最大值不是式（3 ）的最大值啊，而我们想得到式（2 ）的最大值，那怎么办呢？ 现在我们就需要一点想象力了，上面的式（2 ）和式（3 ）不等式可以写成：似然函数 L(θ)>=J(z,Q)，那么我们可以通过不断的最大化这个下界 J，来使得 L(θ) 不断提高，最终达到它的最大值。 ![](http://img.my.csdn.net/uploads/201301/24/1359004484_7944.jpg)
