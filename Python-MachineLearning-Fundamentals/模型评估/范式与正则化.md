# 范式与正则化



正则化是为了防止过拟合

交叉验证是为了选择hyperparameter

# Model Evaluation

# Overfitting
![](https://pic2.zhimg.com/c3fca0a39141f16ae0700b10f44e4909_b.jpg)

(1)对于机器来说，在使用学习算法学习数据的特征的时候，样本数据的特征可以分为局部特征和全局特征，全局特征就是任何你想学习的那个概念所对应的数据都具备的特征，而局部特征则是你用来训练机器的样本里头的数据专有的特征.
(2)在学习算法的作用下，机器在学习过程中是无法区别局部特征和全局特征的，于是机器在完成学习后，除了学习到了数据的全局特征，也可能习得一部分局部特征，而习得的局部特征比重越多，那么新样本中不具有这些局部特征但具有所有全局特征的样本也越多，于是机器无法正确识别符合概念定义的“正确”样本的几率也会上升，也就是所谓的“泛化性”变差，这是过拟合会造成的最大问题.
(3)所谓过拟合，就是指把学习进行的太彻底，把样本数据的所有特征几乎都习得了，于是机器学到了过多的局部特征，过多的由于噪声带来的假特征，造成模型的“泛化性”和识别正确率几乎达到谷点，于是你用你的机器识别新的样本的时候会发现就没几个是正确识别的.
(4)解决过拟合的方法，其基本原理就是限制机器的学习，使机器学习特征时学得不那么彻底，因此这样就可以降低机器学到局部特征和错误特征的几率，使得识别正确率得到优化.
(5)从上面的分析可以看出，要防止过拟合，训练数据的选取也是很关键的，良好的训练数据本身的局部特征应尽可能少，噪声也尽可能小. 

(1)打个形象的比方，给一群天鹅让机器来学习天鹅的特征，经过训练后，知道了天鹅是有翅膀的，天鹅的嘴巴是长长的弯曲的，天鹅的脖子是长长的有点曲度，天鹅的整个体型像一个“2”且略大于鸭子.这时候你的机器已经基本能区别天鹅和其他动物了。
(2)然后，很不巧你的天鹅全是白色的，于是机器经过学习后，会认为天鹅的羽毛都是白的，以后看到羽毛是黑的天鹅就会认为那不是天鹅.
(3)好，来分析一下上面这个例子：(1)中的规律都是对的，所有的天鹅都有的特征，是全局特征；然而，(2)中的规律：天鹅的羽毛是白的.这实际上并不是所有天鹅都有的特征，只是局部样本的特征。机器在学习全局特征的同时，又学习了局部特征，这才导致了不能识别黑天鹅的情况.
![](https://pic2.zhimg.com/afa034d52962681db09b4dc1060f8075_b.png)

## Early stopping
对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）学习算法。Early stopping便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。 
  Early stopping方法的具体做法是，在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。这种做法很符合直观感受，因为accurary都不再提高了，在继续训练也是无 益的，只会提高训练的时间。那么该做法的一个重点便是怎样才认为validation accurary不再提高了呢？并不是说validation accuracy一降下来便认为不再提高了，因为可能经过这个Epoch后，accuracy降低了，但是随后的Epoch又让accuracy又上去 了，所以不能根据一两次的连续降低就判断不再提高。一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了 （Early Stopping）。这种策略也称为“No-improvement-in-n”，n即Epoch的次数，可以根据实际情况取，如10、20、30……
## 数据集扩增
在数据挖掘领域流行着这样的一句话，“有时候往往拥有更多的数据胜过一个好的模型”。因为我们在使用训练数据训练模型，通过这个模型对将来的数据进 行拟合，而在这之间又一个假设便是，训练数据与将来的数据是独立同分布的。即使用当前的训练数据来对将来的数据进行估计与模拟，而更多的数据往往估计与模 拟地更准确。因此，更多的数据有时候更优秀。但是往往条件有限，如人力物力财力的不足，而不能收集到更多的数据，如在进行分类的任务中，需要对数据进行打 标，并且很多情况下都是人工得进行打标，因此一旦需要打标的数据量过多，就会导致效率低下以及可能出错的情况。所以，往往在这时候，需要采取一些计算的方 式与策略在已有的数据集上进行手脚，以得到更多的数据。 
  通俗得讲，数据机扩增即需要得到更多的符合要求的数据，即和已有的数据是独立同分布的，或者近似独立同分布的。一般有以下方法：
- 从数据源头采集更多数据 
- 复制原有数据并加上随机噪声 
- 重采样 
- 根据当前数据集估计数据分布参数，使用该分布产生更多数据等


## Cross Validation:交叉验证

# Norm:范式
> [l0-norm-l1-norm-l2-norm-l-infinity-norm](https://rorasa.wordpress.com/2012/05/13/l0-norm-l1-norm-l2-norm-l-infinity-norm/)
> [differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares](http://www.chioka.in/differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares/)

一般在机器学习的实践中，我们常用的范式就是L1范式
## Types:范式类型
### L0 Norm
### L1 Norm
### L2 Norm
### L-Infinity Norm
## Loss/Error Function:损失函数
L1范式与L2范式常用的场景就是作为损失函数或者代价函数，而使用L1范式的损失函数一般称为LAD(Least Absolute Deviations)，即最小绝对值偏差或者LAE(Least Absolute Errors)，即最小绝对值错误。LAD的目标即是使得目标值$Y_i$与测量值$f(x_i)$之间的绝对值差和最小。
$$
S = \sum_{i=1}^{n}|y_i - f(x_i)|
$$
使用L2范式的损失函数一般又被称为LSE(Least Squares Error)，即最小平方误差：
$$
S = \sum^n_{i=1}(y_i - f(x_i))^2
$$
而LAE与LSE的区别可以总结为：
| L2 loss function    | L1 loss function            |
| ------------------- | --------------------------- |
| Not very robust     | Robust                      |
| Stable solution     | Unstable solution           |
| Always one solution | Possibly multiple solutions |

- Solution uniqueness:是否唯一解
![](http://7xiegq.com1.z0.glb.clouddn.com/L1-norm-and-L2-norm-distance.png)
上图中的绿色的线，即L2范式即是唯一的最短路径的解，而红色的、蓝色的以及黄色的线，即L1范式则是相同长度的解。将这个情况扩展到N维，即可以认为L2范式是有唯一解而L1范式则可以有多解。
- Robustness:鲁棒性，即异常值的影响能力
直观来说，因为LSE使用了L2范式，即将误差值平方求和，这就导致了模型会比LAE存在更大的误差，因此模型与LAE相比会更加地敏感于单个数据点。如果该数据点是个异常值，那么模型会为了更好地拟合这个异常值的情形而导致对于其他值的偏差增大。一般来说，正常的数据点造成的误差值都比较小，而异常数据点造成的误差值会比较大，在LSE中异常数据点对于拟合结果的影响会大于LAE。

- Stable:稳定性，即异常值发生变化时候的影响
在LAD中，如果某个数据点发生了一点水平位移，都可能导致最后的拟合线发生较大的变化。而LSE与之相比则有较大的稳定性，对于数据点任何较小地变化，拟合线也只会变化一点点，即拟合地参数可以看做关于数据的一个持续函数的值。

下图就是一个使用真实的数据与真实的拟合模型构造的结果图：
![](http://7xiegq.com1.z0.glb.clouddn.com/programmatic-L1-vs-L2-visualization.png)

关于上图需要注意两点：
（1）在同一张图中，计入异常点与不计入异常点相比，LAE的两条拟合线差异较大，而LSE则差异较小。
（2）在将异常点从左到右进行移动地过程中，LAE与LSE相比会发生更多的变化。

# Regularization:正则化
  我们都知道，在进行数据挖掘或者机器学习模 型建立的时候，因为在统计学习中，假设数据满足独立同分布（i.i.d，independently and identically distributed），即当前已产生的数据可以对未来的数据进行推测与模拟，因此都是使用历史数据建立模型，即使用已经产生的数据去训练，然后使用该 模型去拟合未来的数据。但是一般独立同分布的假设往往不成立，即数据的分布可能会发生变化（distribution drift），并且可能当前的数据量过少，不足以对整个数据集进行分布估计，因此往往需要防止模型过拟合，提高模型泛化能力。而为了达到该目的的最常见方 法便是：正则化，
正则化(Regularization)是机器学习中一个很重要的防止过拟合的手段，它会为模型的目标函数（objective function）或代价函数（cost function）加上正则项来避免模型参数过度拟合于测试数据。而L1正则与L2正则的区别在于L1是权重的直接和，而L2是权重的平方和。
规则化符合奥卡姆剃刀(Occam's razor)原理，在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型。从贝叶 斯估计的角度来看，规则化项对应于模型的先验概率。民间还有个说法就是，正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项 (regularizer)或惩罚项(penalty term)。加上正则化项之后的监督函数的目标函数为：
$$
w^* = arg min_{w} \sum_i L(y_i,f(x_i;w)) + \lambda \Omega(w)
$$
在上述表达式中，第一项$L(y_i,f(x_i;w))$即是衡量模型对第$i$个样本的预测值$f(x_i;w)$与真实的标签值$y_i$之间的误差。第二项$\lambda \Omega(w)$即是正则化项用于约束我们的模型能够尽量的简单。而L1正则化与L2正则化的差异在于：
| L2 Regularization                        | L1 Regularization                        |
| ---------------------------------------- | ---------------------------------------- |
| Computational efficient due to having analytical solutions | Computational inefficient on non-sparse cases |
| Non-sparse outputs                       | Sparse outputs                           |
| No feature selection                     | Built-in feature selection               |

这里简要解释下上面三个比较项的含义，具体的求值与应用在下文中进行阐述：
- Computational Efficiency:计算性能
- Sparsity:稀疏度
- Built-in Feature Selection:内建的特征选择


## L1 Regularization
## L2 Regularization



