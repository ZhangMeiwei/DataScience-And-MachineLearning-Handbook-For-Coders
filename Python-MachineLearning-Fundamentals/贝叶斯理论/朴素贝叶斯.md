> [深入浅出朴素贝叶斯理论](https://zhuanlan.zhihu.com/p/24602462)归属于笔者的[程序猿的数据科学与机器学习实战手册](https://github.com/wxyyxc1992/DataScience-And-MachineLearning-Handbook-For-Coders)。部分阅读平台对于 MathJax 支持不好，可以查看笔者的[笔记原文](http://62f7d6c2.fromwiz.com/share/s/1yZZr21Yv4w42GorJm0oBXEi0ZrKPU0OAQxF2LDP2t3XZMpw)。更多内容参考[面向程序猿的数据科学与机器学习知识体系及资料合集](https://github.com/wxyyxc1992/DataScience-And-MachineLearning-Handbook-For-Coders/DataScience-Reference)。

![](http://www.machinephilosopher.com/wp-content/uploads/2016/07/NaiveBayes-744x496.jpg)

# 朴素贝叶斯

贝叶斯定理缘起于托马斯 . 贝叶斯（1702-1761 ），一位英国长老会牧师和业余数学家。在他去世后发表的论文 “ 论有关机遇问题的求解 ” 中， 贝叶斯定理的现代形式实际上归因于拉普拉斯（1812 ）。拉普拉斯重新发现了贝叶斯定理，并把它用来解决天体力学、医学甚至法学的问题。但自 19 世纪中叶起，随着频率学派（在下文有时也称作经典统计）的兴起，概率的贝叶斯解释逐渐被统计学主流所拒绝。现代贝叶斯统计学的复兴肇始于 Jeffreys(1939), 在 1950 年代，经过 Wald(1950), Savage(1954), Raiffic&Schlaifer(1961), Lindley(1972), De Finetti(1974) 等人的努力，贝叶斯统计学逐渐发展壮大，并发展出了贝叶斯统计决策理论这个新分支。特别是到 1990 年代以后，随着计算方法 MCMC 在贝叶斯统计领域的广泛应用，解决了贝叶斯统计学长期存在的计算困难的问题，从而推动了贝叶斯统计在理论和应用领域的长足发展。贝叶斯统计学广泛应用于各个学科。就本书的主题而言，从认知学科、政治学到从自然语言处理和社会网络分析，贝叶斯方法都起到了举足轻重的作用。

## 概述

贝叶斯定理，也称为贝叶斯法则现在是概率论教科书的重要内容。一般我们习惯于它的离散（事件）形式： $$ P(A_i|B) = \frac{P(B|A_i)P(A_i)}{ \sum{P(B|A_j)P(A_j)}} \\ A 的后验概率 = \frac{(A 的似然度 \* A 的先验概率 )}{ 标准化常量 }　 $$ 其中

* $B$ 称为观测变量
* $A_i$ 称为参数 / 隐变量
* $P(A_i)$ 称为先验概率，表示在对样本观测前我们关于这个问题已经具有的知识
* $P(A_i|B)$ 称为后验概率，是在进行了新观测之后对原有知识的更新
* $P(B|A_i)$ 称为似然。
* $P(B) = \sum{P(B|A_j)P(A_j)}$ 称为 Evidence，即数据是由该模型得出的证据

贝叶斯定理作为一种概率计算可用于多个领域内进行概率推理。今天，我们用贝叶斯法则过滤垃圾邮件，为网站用户推荐唱片、电影和书籍。它渗透到了互联网、语言和语言处理、人工智能、机器学习、金融、天文学和物理学乃至国家安全等各个领域。这里我们选用一个简单的案例进行分析，假设有方形和圆形的两种盒子，盒子内有红、黄、白三种颜色的球。方盒有 3 个，每个里边有红球 70 只、黄球 10 只、白球 20 只；圆盒有 5 个，每个里边有红球 20 只、黄球 75 只、白球 5 只。现在先任取一个盒子，再从盒中任取一球，能不能通过求得颜色推断它最有可能取自哪个盒子？为表示方便，记方盒 =A，圆盒 =B，红球 =R，黄球 =Y，白球 =W 使用贝叶斯定理进行计算： $$ P(A|R) = \frac{P(R|A)P(A)}{P(R)} =0.6774 $$ 贝叶斯理论最基础的使用就是在分类问题中，也就是所谓的生成式分类器（Generative Classifier ），其基本形式如下所示： $$ p(y = c | \vec{x},\vec{\theta}) \propto p(\vec{x} | y = c, \vec{\theta}) p(y = c | \vec{\theta}) $$ 在训练阶段，我们基于带有标签的训练集的辅助来寻找合适的类条件概率 / 似然概率 $p(\vec{x} | y = c, \vec{\theta})$，并且推导出模型参数 $\vec{\theta}$，其定义了我们期望在某类中出现某类型数据的概率。最后在预测阶段，我们基于类条件概率 / 似然概率来计算数据 $\vec{x}$ 从属于各个类的后验概率，并且选择概率最大的为其预测值。

## Reference

* [贝叶斯集锦（4 ）：贝叶斯统计基础](https://site.douban.com/182577/widget/notes/10567181/note/294041203/)
* [贝叶斯学习及共轭先验](http://blog.csdn.net/acdreamers/article/details/45026459)
* [理解共轭先验](http://wenku.baidu.com/view/a542dbf2770bf78a6529546a.html?st=1)
* [2012 - Machine Learning A Probabilistic Perspective - Chapter 5 - Generative models for discrete data](https://drive.wps.cn/view/l/8a5acb26d91f4008b425430eae8565fb)

# 贝叶斯理论思维模式

在我们孩提时代，爸妈希望教会我们某个词汇的含义时，他们首先会给我们展示很多的正例。譬如对于狗这个单词，爸妈可能会说：看那条狗狗好可爱，或者，小心狗狗。不过爸妈不会像机器一样给我们展示所谓的负例，他们不会指着一只猫说：这货不是狗，最多就是当孩子们认错的时候，父母会予以纠正。心理学家研究表明，人们可以单纯地从正例中学习概念，而不一定需要负例的介入。而这种认知单词的学习过程可以抽象概括为所谓的概念学习（Concept Learning ），在某些意义上很类似于二元分类。譬如我们可以定义当 $x$ 为某个概念 C 的实例时 $f(x) = 1$，否则 $f(x) = 0$。而学习的过程即是构建这个指示函数 $f$，该函数定义了哪些元素属于概念 C。当我们允许这个函数具有一定的不确定性时，我们就可以通过概率计算得出所谓的模糊集（Fuzzy Set ）。还需要提到的是，标准的二维分类是同时需要正负例存在的，不过我们也可以单纯地从正例中学习。阐述完了基本的概念，接下来我们会以一个简单的数字游戏来进行形象化的说明，这里我们随便选定几个数学上的概念作为学习目标。譬如我们可以将概念 C 定义为所有的素数，或者介于 1~10 之间的数字。然后给你多组随机从 C 中抽样出的正数序列：$D = \{x_1,...,x_N\}$ ，然后给你一个新的测试序列 $ \widetilde{x} $ 让你判断其应该归属于哪个概念。

![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2016/12/3/QQ20161228-0.png)

上图四组对比数据分别显示了给不同的组选定不同的观测集合时他们推导出的概念 C 的数字分布。前两行是分别展示了 $D = \{16\}$ 与 $D = \{60\}$，会发现得出的结果非常分散（这里选定的数字范围为 1~100）。而第三行中观测数据为 $D = \{16，8 ， 2，64\}$ ，人们得出了一定的规律，即选定了 2 的方幂值。而最后一行中给出的观测数据是 $D = {16，23 ， 19，20}$ ，人们得出的规律是选定靠近 20 的数字。我们来复盘每个组的思考过程，譬如当首先给出 $16$ 作为观测数据时，人们可能会选择 17？因为 17 离 16 最近，也有可能会选择 6，因为它们的个位数都是 6. 当然也有可能是 32，因为它们都是 2 的方幂值，不过估计是没啥人会选择 99 的。从这样简单地思考过程我们可以得出一个结论，显而易见的部分数字被选中的概率是大于其他数字的，这种概率就可以表示为某个概率分布：$p(\widetilde{x} | D)$ 。这个概率就是所谓的后验概率，表示了在给定观测值 $D$ 的情况下每个数字属于 D 的概念集 $\widetilde{x} ~ C$ 的概率。接下来如果继续给出 $8,2,64$ 作为正例，那么我们会猜测隐藏的概念为 2 的方幂值，这种思考过程就是典型的归纳（Induction ）。而如果继续给出 $23,19,20$ 作为正例，那么我们会得出另一个完全不同的泛化梯度（Generalization Gradient ）的结果。机器学习的任务就是将上述思考的过程转化为机器计算，经典的在让机器进行数学归纳的方法就是我们先预置很多概念的假设空间 $H$（Hypothesis Space ），譬如：奇数、偶数、1~100 之间的数字、2 的方幂、所有以 6 结尾的数字等等。而与观测值 $D$ 相符的 $H$ 的子集称为样本空间（Version Space ）。譬如在上面的思考过程中，随着样本空间的增长我们越发坚定了对于某个概念的信心。不过样本空间往往会很多且重复，譬如上文中如果 $D={16}$，其与很多假设空间都存在一致的样本空间，又该如何抉择呢？

## Likelihood ：似然

我们首先来讨论下为什么当我们观测到 $D={16,8,2,64}$ 时更倾向于认为假设空间是所有 2 的方幂值的集合，而不是笃定假设空间是所有偶数的集合。虽然两个假设空间都符合我们的观测结果，但是归纳的过程中我们会尽量避免可疑的巧合（Suspicious Coincidences ）。如果我们认为假设空间是所有偶数的集合，那么又该如何说服自己这些数字都是 2 的方幂值呢？为了更方便的形式化讨论这个现象，我们假设从某个假设空间中随机取值的概率分布为均匀分布，可以推导出从假设空间中进行 N 次取值得到观测集合的概率为： $$ p(D|h) = [\frac{1}{size(h)}]^N = [\frac{1}{|h|}]^N $$ 对于这个等式最形象化的解释就是奥卡姆剃刀原则（Occam’s razor ），我们倾向于选择符合观测值的最小 / 最简的假设空间。在 $D=\{16\}$ 的情况下，如果假设空间为 2 的方幂值，则仅有 6 个符合条件的数字，推导出 $p(D|h*{two}) = 1/6$。而如果是所有的偶数集合，$p(D|h*{even}) = 1/50$ 。显而易见 $h*two > h_even$，如果观测序列中有 4 个数值，则 $h*{two} = (1/6)^4 = 7.7 * 10^{-4}$，然而 $h\_{even} = (1/50)^4 = 1.6*10^{-7}$，不同的假设空间的概率值差异越发的大了。因此我们会认为 $D = \{16，8 ， 2，64\}$ 这个观测序列是来自于 2 的方幂值这个假设空间而不是所有的偶数集合这个假设空间。

## Prior ：先验

前一节我们讨论了所谓似然的概念，当观测到 $D = \{16，8 ， 2，64\}$ 时我们会倾向于认为其采样于 2 的方幂值这个集合，不过为啥不是 $h' = 除了 32 之外的 2 的方幂值 $ 这个似然概率更大的集合呢？直观来看就是 $h' = 除了 32 之外的 2 的方幂值 $ 这个假设与常规思维不符，而对于这样奇特的思维我们可以赋予其较低的先验概率值来降低其最终得到的后验概率。总计而言，贝叶斯理论中概率并不需要频率解释，先验分布也可以称为主观概率，是根据经验对随机现象的发生可能性的一种看法或者信念。统计学家萨维奇曾给出过一个著名的女士品茶的例子：一位常喝牛奶加茶的女士说她可以分辨在杯中先加入的是茶还是奶。连续做了十次实验，她都说对了。显然这来自于她的经验而非猜测。我们在日常生活中也经常使用基于经验或者信念的主观的概率陈述。比如说，天气预报里说明天（8 月 3 日）降水概率 30%，就是关于 “ 明日降水 ” 这个事件的一种信念，因为作为 8 月 3 日的明天是不可重复的，自然也就没有频率意义。再比如说，医生认为对某位病人进行手术的成功可能性为 80%，也是根据自己的经验而具有的的信念，而非在这位病人身上反复进行试验的频率结果。 把 θ 看做随机变量，进而提出先验分布，在许多情况下是合理的。比如工厂产品的合格率每一天都有波动，可以看做随机变量；明天的降水概率虽然是几乎不动的，但这是基于经验和规律提出来的概率陈述，也可以看做随机变量。尽管我们使用后验分布来进行推理，但先验分布的选取也是很重要的。常见的先验分布类型包括：

* 无信息先验（Noninformative Priors ） 无信息先验只包含了参数的模糊的或者一般的信息，是对后验分布影响最小的先验分布。很多人愿意选取无信息先验，因为这种先验与其它 “ 主观 ” 的先验相比更接近 “ 客观 ”。通常，我们把均匀分布作为无信息先验来使用，这相当于在参数所有的可能值上边指派了相同的似然。但是无先验信息的使用也要慎重，比如有些情况下会导致不恰当的后验分布（如不可积分的后验概率密度）。
* Jeffreys 先验 (Jeffreys’ Prior) Jeffreys 提出的选取先验分布的原则是一种不变原理，采用 Fisher 信息阵的平方根作为 θ 的无信息先验分布。较好地解决了无信息先验中的一个矛盾，即若对参数 θ 选用均匀分布，则其函数 g(θ) 往往不是均匀分布。
* 信息先验（Informative Priors ） 根据以前的经验、研究或专家经验得到的先验分布。
* 共轭先验（Conjugate Priors ） 共轭先验是指先验分布和后验分布来自同一个分布族的情况，就是说先验和后验有相同的分布形式（当然，参数是不同的）。这些共轭先验是结合似然的形式推导出来的。共轭先验是经常被使用的一种先验分布形式，原因在于数学处理和计算上的方便性，同时后验分布的一些参数也可以有很好的解释。

## Posterior ：后验

后验值即为似然乘以先验再进行归一化，对于这里的数字游戏： $$ p(h|D) = \frac{p(D|h)p(h)}{\sum*{h' \in H}p(D,h')} = \frac {p(h)\amalg(D\in h) / |h|^N} {\sum*{h' \in H}p(h')\amalg(D \in h') / |h'|^N} $$ 其中 $\amalg(D\in h) $ 当且仅当 $D$ 中所有数据都属于假设空间 $h$ 时取 1，其他情况下取 0。 ![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2016/12/3/QQ20161228-011.png) 上图展示了观测值为 $\{16\}$ 情况下对应的先验、似然与后验值，其中后验值是先验乘以似然的结果。对于大部分概念而言，先验都是一致的，此时后验值取决于似然。不过对于上文中提及的 $h' = 除了 32 之外的 2 的方幂值 $，其先验概率取值极地，因此虽然其有着不错的似然，其最终得出的后验概率值还是很小的。而观测值 $D = \{ 16,8,2,64 \}$ 时，其先验、似然与后验如下图所示： ![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2016/12/3/QQ20161228-022.png) 总体而言，当我们具有足够数目的数据时，后验概率 $p(h|D)$ 会在某个概念上达到峰值，求取目标假设空间的过程（预测阶段）就可以引入 MAP（Maximum a Posterior ）估计： $$ \hat{h} ^ {MAP} = argmax_{h} p(D|h)p(h) = argmax_h [log p(D|h) + log p(h)] $$ 而当观测数据足够多时，似然值的影响会远大于先验，此时 MAP 就近似于最大似然估计 MLE（Maximum Likelihood Estimate ）。

# 共轭先验

## 贝叶斯理论应用

![](http://bridge-global.com/blog/wp-content/uploads/2016/02/download.png)

在真实的机器学习任务中，我们往往是需要在训练集上得出似然函数值。譬如对上述盒子与球模型中，我们将问题重新叙述为：我们的训练集中有多个方形与圆形的盒子，每个盒子中含有不定数量的红黄白三种颜色的小球。那么此时问题就抽象为了 : 当你观察一个事件 $X$，你预估计并给出其内部参数 $\theta$，表示你对于事件 $X$ 发生的置信程度。
