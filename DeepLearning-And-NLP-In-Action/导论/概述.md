# Deep Learning

> “Deep learning is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers, with complex structures or otherwise, composed of multiple non-linear transformations.”

![](http://rdc.hundsun.com/portal/data/upload/201703/f_44148eb8dbcb119563182ec26adad049.gif)

传统的机器学习主要分两部分，一部分是特征提取，需要人工从数据中提取出特征，并且想办法用计算机能理解的格式来表达这些特征。另外一部分是是模型，使用计算机能理解的特征数据来训练模型。人工提取的特征慢，而且很容易遗漏。深度学习能够从原始数据中学习出特征，然后用来训练模型。理论上来说 参数越多的模型复杂度越高 “ 容量 ” (capacity) 越大 , 这意味着它能完成吏复杂的学习任务 但一股倩形下 , 复杂模型的训练效率低 易陷入过拟合 , 因此难以受到人们青睐 而随着云计算、 大数据时代的到来 计算能力 的大幅提高可缓解训练低效性 训练数据的大幅增加则可降低过拟合 f 因此 , 以 " 深度学习 ” (deep1earning) 为代表的复杂模型开始受到人们的关注。 典型的深度学习模型就是很深层的神经网络 . 显然 , 对神经网络模型 提高容量的一个简单办法是增加隐层的数且 隐层多工 相应的神经元连接机 阈值等参数就会更多模型复杂度也可通过单纯增加隐层神经元的数目来实玑前面我们谈到过 , 单隐层的多层呤前馈网络己具有很强大的学习能力 ; 但从增加模型复杂度的角觑肴 增加隐层的数目显然比增加隐层神经元的数目更眦因为增加隐层数不仅增加丁拥有激活函数的神经元数目 , 还增加了激活函数嵌套的层数 然而 , 多隐层神经网络难以直接用经典算法恻如标准 BP 算淘进行训练 , 因为误差在多隐层内逆传播盹 往往会 “ 发散 ” (diverge) 而不能收敛到稳定状态

# Deep Architectures

![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2016/8/1/A6B65EE8-F5B8-4C13-9F86-6573A6256354.png)

# Tools

![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2016/8/1/saxaxa.png) ![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2016/8/1/cadsfsd.png)
